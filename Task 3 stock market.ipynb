
Let's Grow More(LGMVIP) - "DATA SCIENCE INTERN"
BEGINNER LEVEL TASK
TASK-2 - Stock Market Prediction And Forecasting Using Stacked LSTM
DatasetLink : https://raw.githubusercontent.com/mwitiderrick/stockprice/master/NSE-TATAGLOBAL.csv
import numpy as np
import pandas as pd
import datetime
import math
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense , LSTM
from sklearn.metrics import mean_squared_error
Dataset_link='https://raw.githubusercontent.com/mwitiderrick/stockprice/master/NSE-TATAGLOBAL.csv'
df= pd.read_csv(Dataset_link, parse_dates=True,)
df.reset_index()
df.head(10)
Date	Open	High	Low	Last	Close	Total Trade Quantity	Turnover (Lacs)
0	2018-09-28	234.05	235.95	230.20	233.50	233.75	3069914	7162.35
1	2018-09-27	234.55	236.80	231.10	233.80	233.25	5082859	11859.95
2	2018-09-26	240.00	240.00	232.50	235.00	234.25	2240909	5248.60
3	2018-09-25	233.30	236.75	232.00	236.25	236.10	2349368	5503.90
4	2018-09-24	233.55	239.20	230.75	234.00	233.30	3423509	7999.55
5	2018-09-21	235.00	237.00	227.95	233.75	234.60	5395319	12589.59
6	2018-09-19	235.95	237.20	233.45	234.60	234.90	1362058	3202.78
7	2018-09-18	237.90	239.25	233.50	235.50	235.05	2614794	6163.70
8	2018-09-17	233.15	238.00	230.25	236.40	236.60	3170894	7445.41
9	2018-09-14	223.45	236.70	223.30	234.00	233.95	6377909	14784.50
df.sample(10)
Date	Open	High	Low	Last	Close	Total Trade Quantity	Turnover (Lacs)
1814	2011-06-06	92.30	97.00	92.10	92.85	93.15	337230	315.75
1789	2011-07-11	100.10	102.50	99.50	101.75	101.45	893861	908.48
1828	2011-05-17	97.25	98.25	96.00	96.40	96.35	207063	200.97
1076	2014-05-22	154.50	157.40	153.40	155.75	155.55	3246095	5062.35
214	2017-11-17	252.25	260.80	250.80	258.95	259.90	6857865	17611.63
608	2016-04-20	120.20	123.20	119.55	121.80	121.85	1572869	1904.59
1567	2012-05-31	107.55	109.30	106.35	108.35	107.60	1024425	1104.58
144	2018-02-28	273.00	277.00	272.00	275.40	275.50	2808234	7727.14
656	2016-02-04	123.10	123.40	120.40	122.50	121.60	1120485	1366.68
947	2014-12-03	164.90	165.90	162.75	162.95	163.25	2826458	4640.58
df.columns
Index(['Date', 'Open', 'High', 'Low', 'Last', 'Close', 'Total Trade Quantity',
       'Turnover (Lacs)'],
      dtype='object')
df.shape
(2035, 8)
df.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 2035 entries, 0 to 2034
Data columns (total 8 columns):
 #   Column                Non-Null Count  Dtype  
---  ------                --------------  -----  
 0   Date                  2035 non-null   object 
 1   Open                  2035 non-null   float64
 2   High                  2035 non-null   float64
 3   Low                   2035 non-null   float64
 4   Last                  2035 non-null   float64
 5   Close                 2035 non-null   float64
 6   Total Trade Quantity  2035 non-null   int64  
 7   Turnover (Lacs)       2035 non-null   float64
dtypes: float64(6), int64(1), object(1)
memory usage: 127.3+ KB
df.isnull().sum()
Date                    0
Open                    0
High                    0
Low                     0
Last                    0
Close                   0
Total Trade Quantity    0
Turnover (Lacs)         0
dtype: int64
df.describe()
Open	High	Low	Last	Close	Total Trade Quantity	Turnover (Lacs)
count	2035.000000	2035.000000	2035.000000	2035.000000	2035.00000	2.035000e+03	2035.000000
mean	149.713735	151.992826	147.293931	149.474251	149.45027	2.335681e+06	3899.980565
std	48.664509	49.413109	47.931958	48.732570	48.71204	2.091778e+06	4570.767877
min	81.100000	82.800000	80.000000	81.000000	80.95000	3.961000e+04	37.040000
25%	120.025000	122.100000	118.300000	120.075000	120.05000	1.146444e+06	1427.460000
50%	141.500000	143.400000	139.600000	141.100000	141.25000	1.783456e+06	2512.030000
75%	157.175000	159.400000	155.150000	156.925000	156.90000	2.813594e+06	4539.015000
max	327.700000	328.750000	321.650000	325.950000	325.75000	2.919102e+07	55755.080000
plt.figure(figsize=(10,6))
df['Close'].plot(kind='line',figsize=(16,7),color='r',label="Closing Price")

plt.ylabel("Price")
plt.legend(loc="upper right")
plt.title("Change in closing price over the years")
plt.grid()

plt.figure(figsize=(10,6))
df['Open'].plot(kind='line',figsize=(16,7),color='y',label="Opening Price")

plt.ylabel("Price")
plt.legend(loc="upper left")
plt.title("Change in opening price over the years")
plt.grid()

df1=df.reset_index()['Close']
df1
0       233.75
1       233.25
2       234.25
3       236.10
4       233.30
         ...  
2030    118.65
2031    117.60
2032    120.65
2033    120.90
2034    121.55
Name: Close, Length: 2035, dtype: float64
plt.figure(figsize=(12,6))
sns.heatmap(df.corr(),annot=True,cmap='BuPu')
<AxesSubplot:>

plt.figure(figsize=(11,5))
plt.subplot(1,2,1)
sns.boxplot(data=df,y='Total Trade Quantity',color='green')
plt.subplot(1,2,2)
sns.boxplot(data=df,y='Turnover (Lacs)',color='red')
<AxesSubplot:ylabel='Turnover (Lacs)'>

fig=plt.figure(figsize=(7,6))
plt.scatter(df['Total Trade Quantity'],df['Turnover (Lacs)'], alpha=0.5, edgecolor='r', color='cyan')
plt.xlabel("Trade Quantity (in 100000)")
plt.ylabel("Turnover (in lacs)")
plt.title(" Selling Units Vs Turnover")
plt.show()

training_set= df[['Open']]
training_set=pd.DataFrame(training_set)
training_set
Open
0	234.05
1	234.55
2	240.00
3	233.30
4	233.55
...	...
2030	117.60
2031	120.10
2032	121.80
2033	120.30
2034	122.10
2035 rows Ã— 1 columns

scaler=MinMaxScaler(feature_range=(0,1))
training_set_scaler=scaler.fit_transform(np.array(df1).reshape(-1,1))
training_set_scaler
array([[0.62418301],
       [0.62214052],
       [0.62622549],
       ...,
       [0.1621732 ],
       [0.16319444],
       [0.16584967]])
train_size1= int(len(training_set_scaler)*0.65)
test_size1=int(len(training_set_scaler))-train_size1
train_data1,test_data1=training_set_scaler[0:train_size1,:],training_set_scaler[train_size1:len(df),:1] 
train_size1
1322
def create_dataset(dataset,time_step=1):
    dataX, dataY = [], []
    for i in range(len(dataset)-time_step-1):
        a = dataset[i:(i+time_step), 0]   ###i=0, 0,1,2,3-----99   100 
        dataX.append(a)
        dataY.append(dataset[i + time_step, 0])
    return np.array(dataX), np.array(dataY)
time_step=100


x_train, y_train=create_dataset(train_data1, time_step)
x_test, y_test= create_dataset(test_data1, time_step)
print(x_train.shape,y_train.shape)
(1221, 100) (1221,)
x_test.shape
(612, 100)
y_test.shape
(612,)
x_train = x_train.reshape(x_train.shape[0],x_train.shape[1] , 1)
x_test = x_test.reshape(x_test.shape[0],x_test.shape[1] , 1)
model = Sequential()
model.add(LSTM(50, return_sequences=True, input_shape=(100,1)))
model.add(LSTM(50, return_sequences=True, input_shape=(100,1)))
model.add(LSTM(50))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam', metrics='acc')
model.summary()
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 lstm (LSTM)                 (None, 100, 50)           10400     
                                                                 
 lstm_1 (LSTM)               (None, 100, 50)           20200     
                                                                 
 lstm_2 (LSTM)               (None, 50)                20200     
                                                                 
 dense (Dense)               (None, 1)                 51        
                                                                 
=================================================================
Total params: 50,851
Trainable params: 50,851
Non-trainable params: 0
_________________________________________________________________
model.fit(x_train, y_train, validation_data = (x_test, y_test), epochs = 75, batch_size = 64, verbose = 1)
Epoch 1/75
20/20 [==============================] - 40s 899ms/step - loss: 0.0167 - acc: 0.0000e+00 - val_loss: 0.0052 - val_acc: 0.0016
Epoch 2/75
20/20 [==============================] - 12s 576ms/step - loss: 0.0024 - acc: 8.1900e-04 - val_loss: 8.8966e-04 - val_acc: 0.0016
Epoch 3/75
20/20 [==============================] - 11s 533ms/step - loss: 0.0017 - acc: 8.1900e-04 - val_loss: 8.9602e-04 - val_acc: 0.0016
Epoch 4/75
20/20 [==============================] - 10s 528ms/step - loss: 0.0016 - acc: 8.1900e-04 - val_loss: 9.9247e-04 - val_acc: 0.0016
Epoch 5/75
20/20 [==============================] - 11s 530ms/step - loss: 0.0015 - acc: 8.1900e-04 - val_loss: 0.0011 - val_acc: 0.0016
Epoch 6/75
20/20 [==============================] - 11s 542ms/step - loss: 0.0015 - acc: 8.1900e-04 - val_loss: 0.0015 - val_acc: 0.0016
Epoch 7/75
20/20 [==============================] - 11s 558ms/step - loss: 0.0014 - acc: 8.1900e-04 - val_loss: 0.0010 - val_acc: 0.0016
Epoch 8/75
20/20 [==============================] - 11s 536ms/step - loss: 0.0013 - acc: 8.1900e-04 - val_loss: 9.7105e-04 - val_acc: 0.0016
Epoch 9/75
20/20 [==============================] - 12s 603ms/step - loss: 0.0013 - acc: 8.1900e-04 - val_loss: 0.0015 - val_acc: 0.0016
Epoch 10/75
20/20 [==============================] - 11s 566ms/step - loss: 0.0013 - acc: 8.1900e-04 - val_loss: 9.4616e-04 - val_acc: 0.0016
Epoch 11/75
20/20 [==============================] - 11s 535ms/step - loss: 0.0013 - acc: 8.1900e-04 - val_loss: 0.0017 - val_acc: 0.0016
Epoch 12/75
20/20 [==============================] - 11s 544ms/step - loss: 0.0012 - acc: 8.1900e-04 - val_loss: 8.6889e-04 - val_acc: 0.0016
Epoch 13/75
20/20 [==============================] - 11s 537ms/step - loss: 0.0011 - acc: 8.1900e-04 - val_loss: 8.3324e-04 - val_acc: 0.0016
Epoch 14/75
20/20 [==============================] - 11s 532ms/step - loss: 0.0010 - acc: 8.1900e-04 - val_loss: 8.6244e-04 - val_acc: 0.0016
Epoch 15/75
20/20 [==============================] - 11s 542ms/step - loss: 0.0011 - acc: 8.1900e-04 - val_loss: 8.5002e-04 - val_acc: 0.0016
Epoch 16/75
20/20 [==============================] - 12s 612ms/step - loss: 9.9249e-04 - acc: 8.1900e-04 - val_loss: 8.0903e-04 - val_acc: 0.0016
Epoch 17/75
20/20 [==============================] - 13s 634ms/step - loss: 9.5157e-04 - acc: 8.1900e-04 - val_loss: 6.7198e-04 - val_acc: 0.0016
Epoch 18/75
20/20 [==============================] - 11s 554ms/step - loss: 9.7298e-04 - acc: 8.1900e-04 - val_loss: 7.4621e-04 - val_acc: 0.0016
Epoch 19/75
20/20 [==============================] - 11s 531ms/step - loss: 0.0015 - acc: 8.1900e-04 - val_loss: 0.0018 - val_acc: 0.0016
Epoch 20/75
20/20 [==============================] - 13s 637ms/step - loss: 9.3476e-04 - acc: 8.1900e-04 - val_loss: 6.5499e-04 - val_acc: 0.0016
Epoch 21/75
20/20 [==============================] - 11s 567ms/step - loss: 0.0014 - acc: 8.1900e-04 - val_loss: 6.7509e-04 - val_acc: 0.0016
Epoch 22/75
20/20 [==============================] - 11s 547ms/step - loss: 0.0018 - acc: 8.1900e-04 - val_loss: 9.9573e-04 - val_acc: 0.0016
Epoch 23/75
20/20 [==============================] - 11s 531ms/step - loss: 8.1852e-04 - acc: 8.1900e-04 - val_loss: 7.5184e-04 - val_acc: 0.0016
Epoch 24/75
20/20 [==============================] - 11s 529ms/step - loss: 7.5189e-04 - acc: 8.1900e-04 - val_loss: 8.5909e-04 - val_acc: 0.0016
Epoch 25/75
20/20 [==============================] - 11s 542ms/step - loss: 7.3760e-04 - acc: 8.1900e-04 - val_loss: 7.1944e-04 - val_acc: 0.0016
Epoch 26/75
20/20 [==============================] - 11s 538ms/step - loss: 7.1448e-04 - acc: 8.1900e-04 - val_loss: 6.4777e-04 - val_acc: 0.0016
Epoch 27/75
20/20 [==============================] - 11s 531ms/step - loss: 7.1889e-04 - acc: 8.1900e-04 - val_loss: 6.2468e-04 - val_acc: 0.0016
Epoch 28/75
20/20 [==============================] - 11s 577ms/step - loss: 6.8147e-04 - acc: 8.1900e-04 - val_loss: 9.9908e-04 - val_acc: 0.0016
Epoch 29/75
20/20 [==============================] - 12s 617ms/step - loss: 8.8138e-04 - acc: 8.1900e-04 - val_loss: 0.0012 - val_acc: 0.0016
Epoch 30/75
20/20 [==============================] - 12s 627ms/step - loss: 8.3634e-04 - acc: 8.1900e-04 - val_loss: 8.2133e-04 - val_acc: 0.0016
Epoch 31/75
20/20 [==============================] - 12s 618ms/step - loss: 6.8271e-04 - acc: 8.1900e-04 - val_loss: 6.4301e-04 - val_acc: 0.0016
Epoch 32/75
20/20 [==============================] - 12s 618ms/step - loss: 6.2931e-04 - acc: 8.1900e-04 - val_loss: 7.1893e-04 - val_acc: 0.0016
Epoch 33/75
20/20 [==============================] - 14s 718ms/step - loss: 6.3421e-04 - acc: 8.1900e-04 - val_loss: 5.7048e-04 - val_acc: 0.0016
Epoch 34/75
20/20 [==============================] - 11s 577ms/step - loss: 7.2572e-04 - acc: 8.1900e-04 - val_loss: 5.4896e-04 - val_acc: 0.0016
Epoch 35/75
20/20 [==============================] - 11s 535ms/step - loss: 6.6246e-04 - acc: 8.1900e-04 - val_loss: 6.1585e-04 - val_acc: 0.0016
Epoch 36/75
20/20 [==============================] - 11s 529ms/step - loss: 6.7028e-04 - acc: 8.1900e-04 - val_loss: 5.4539e-04 - val_acc: 0.0016
Epoch 37/75
20/20 [==============================] - 11s 539ms/step - loss: 5.9239e-04 - acc: 8.1900e-04 - val_loss: 6.1434e-04 - val_acc: 0.0016
Epoch 38/75
20/20 [==============================] - 11s 541ms/step - loss: 5.7342e-04 - acc: 8.1900e-04 - val_loss: 6.8991e-04 - val_acc: 0.0016
Epoch 39/75
20/20 [==============================] - 11s 549ms/step - loss: 5.6113e-04 - acc: 8.1900e-04 - val_loss: 4.7917e-04 - val_acc: 0.0016
Epoch 40/75
20/20 [==============================] - 11s 532ms/step - loss: 6.1910e-04 - acc: 8.1900e-04 - val_loss: 5.2825e-04 - val_acc: 0.0016
Epoch 41/75
20/20 [==============================] - 11s 538ms/step - loss: 6.0119e-04 - acc: 8.1900e-04 - val_loss: 5.4779e-04 - val_acc: 0.0016
Epoch 42/75
20/20 [==============================] - 11s 538ms/step - loss: 5.5540e-04 - acc: 8.1900e-04 - val_loss: 5.4936e-04 - val_acc: 0.0016
Epoch 43/75
20/20 [==============================] - 11s 534ms/step - loss: 5.1906e-04 - acc: 8.1900e-04 - val_loss: 5.6550e-04 - val_acc: 0.0016
Epoch 44/75
20/20 [==============================] - 11s 538ms/step - loss: 5.1043e-04 - acc: 8.1900e-04 - val_loss: 7.6912e-04 - val_acc: 0.0016
Epoch 45/75
20/20 [==============================] - 11s 536ms/step - loss: 5.2462e-04 - acc: 8.1900e-04 - val_loss: 4.3025e-04 - val_acc: 0.0016
Epoch 46/75
20/20 [==============================] - 11s 536ms/step - loss: 5.6803e-04 - acc: 8.1900e-04 - val_loss: 6.8644e-04 - val_acc: 0.0016
Epoch 47/75
20/20 [==============================] - 11s 541ms/step - loss: 9.6459e-04 - acc: 8.1900e-04 - val_loss: 7.7619e-04 - val_acc: 0.0016
Epoch 48/75
20/20 [==============================] - 11s 545ms/step - loss: 5.3019e-04 - acc: 8.1900e-04 - val_loss: 4.5101e-04 - val_acc: 0.0016
Epoch 49/75
20/20 [==============================] - 11s 537ms/step - loss: 5.0576e-04 - acc: 8.1900e-04 - val_loss: 4.0941e-04 - val_acc: 0.0016
Epoch 50/75
20/20 [==============================] - 11s 547ms/step - loss: 5.3908e-04 - acc: 8.1900e-04 - val_loss: 6.5609e-04 - val_acc: 0.0016
Epoch 51/75
20/20 [==============================] - 11s 538ms/step - loss: 4.7966e-04 - acc: 8.1900e-04 - val_loss: 4.5303e-04 - val_acc: 0.0016
Epoch 52/75
20/20 [==============================] - 11s 537ms/step - loss: 4.2461e-04 - acc: 8.1900e-04 - val_loss: 4.4048e-04 - val_acc: 0.0016
Epoch 53/75
20/20 [==============================] - 11s 536ms/step - loss: 4.3347e-04 - acc: 8.1900e-04 - val_loss: 4.1135e-04 - val_acc: 0.0016
Epoch 54/75
20/20 [==============================] - 13s 646ms/step - loss: 4.1797e-04 - acc: 8.1900e-04 - val_loss: 3.4338e-04 - val_acc: 0.0016
Epoch 55/75
20/20 [==============================] - 12s 588ms/step - loss: 4.3309e-04 - acc: 8.1900e-04 - val_loss: 8.4545e-04 - val_acc: 0.0016
Epoch 56/75
20/20 [==============================] - 11s 530ms/step - loss: 6.4592e-04 - acc: 8.1900e-04 - val_loss: 3.7745e-04 - val_acc: 0.0016
Epoch 57/75
20/20 [==============================] - 11s 535ms/step - loss: 4.9381e-04 - acc: 8.1900e-04 - val_loss: 3.6815e-04 - val_acc: 0.0016
Epoch 58/75
20/20 [==============================] - 10s 527ms/step - loss: 3.9979e-04 - acc: 8.1900e-04 - val_loss: 3.2008e-04 - val_acc: 0.0016
Epoch 59/75
20/20 [==============================] - 11s 531ms/step - loss: 3.6480e-04 - acc: 8.1900e-04 - val_loss: 5.1364e-04 - val_acc: 0.0016
Epoch 60/75
20/20 [==============================] - 11s 529ms/step - loss: 4.1606e-04 - acc: 8.1900e-04 - val_loss: 3.1054e-04 - val_acc: 0.0016
Epoch 61/75
20/20 [==============================] - 11s 531ms/step - loss: 4.0322e-04 - acc: 8.1900e-04 - val_loss: 3.6537e-04 - val_acc: 0.0016
Epoch 62/75
20/20 [==============================] - 11s 531ms/step - loss: 3.5620e-04 - acc: 8.1900e-04 - val_loss: 3.5757e-04 - val_acc: 0.0016
Epoch 63/75
20/20 [==============================] - 11s 534ms/step - loss: 3.4160e-04 - acc: 8.1900e-04 - val_loss: 3.6011e-04 - val_acc: 0.0016
Epoch 64/75
20/20 [==============================] - 10s 526ms/step - loss: 3.5162e-04 - acc: 8.1900e-04 - val_loss: 2.8030e-04 - val_acc: 0.0016
Epoch 65/75
20/20 [==============================] - 11s 531ms/step - loss: 3.2541e-04 - acc: 8.1900e-04 - val_loss: 3.2999e-04 - val_acc: 0.0016
Epoch 66/75
20/20 [==============================] - 11s 578ms/step - loss: 3.1862e-04 - acc: 8.1900e-04 - val_loss: 2.7344e-04 - val_acc: 0.0016
Epoch 67/75
20/20 [==============================] - 11s 532ms/step - loss: 3.3063e-04 - acc: 8.1900e-04 - val_loss: 3.2640e-04 - val_acc: 0.0016
Epoch 68/75
20/20 [==============================] - 10s 524ms/step - loss: 3.4467e-04 - acc: 8.1900e-04 - val_loss: 2.9315e-04 - val_acc: 0.0016
Epoch 69/75
20/20 [==============================] - 10s 524ms/step - loss: 2.9892e-04 - acc: 8.1900e-04 - val_loss: 3.0314e-04 - val_acc: 0.0016
Epoch 70/75
20/20 [==============================] - 11s 531ms/step - loss: 2.9372e-04 - acc: 8.1900e-04 - val_loss: 2.7479e-04 - val_acc: 0.0016
Epoch 71/75
20/20 [==============================] - 11s 533ms/step - loss: 3.4535e-04 - acc: 8.1900e-04 - val_loss: 3.6353e-04 - val_acc: 0.0016
Epoch 72/75
20/20 [==============================] - 10s 526ms/step - loss: 3.4223e-04 - acc: 8.1900e-04 - val_loss: 3.3352e-04 - val_acc: 0.0016
Epoch 73/75
20/20 [==============================] - 11s 535ms/step - loss: 3.9170e-04 - acc: 8.1900e-04 - val_loss: 2.4112e-04 - val_acc: 0.0016
Epoch 74/75
20/20 [==============================] - 10s 522ms/step - loss: 3.5736e-04 - acc: 8.1900e-04 - val_loss: 2.5138e-04 - val_acc: 0.0016
Epoch 75/75
20/20 [==============================] - 11s 534ms/step - loss: 2.8515e-04 - acc: 8.1900e-04 - val_loss: 2.7281e-04 - val_acc: 0.0016
<keras.callbacks.History at 0x1e49606b850>
train_predict1=model.predict(x_train)
test_predict1=model.predict(x_test)
#Transformback to original form
train_predict1=scaler.inverse_transform(train_predict1)
test_predict1=scaler.inverse_transform(test_predict1)
39/39 [==============================] - 8s 110ms/step
20/20 [==============================] - 2s 102ms/step
math.sqrt(mean_squared_error(y_train,train_predict1))
167.89318225081448
math.sqrt(mean_squared_error(y_test,test_predict1))
116.68292563778971
### Plotting 
# shift train predictions for plotting
look_back=100
trainPredictPlot = np.empty_like(training_set_scaler)
trainPredictPlot[:, :] = np.nan
trainPredictPlot[look_back:len(train_predict1)+look_back, :] = train_predict1

# shift test predictions for plotting
testPredictPlot = np.empty_like(training_set_scaler)
testPredictPlot[:, :] = np.nan
testPredictPlot[len(train_predict1)+(look_back*2)+1:len(df1)-1, :] = test_predict1

# plot baseline and predictions
plt.figure(figsize=(10,5))
plt.plot(scaler.inverse_transform(training_set_scaler))
plt.plot(trainPredictPlot)
plt.plot(testPredictPlot)
plt.show()

 
